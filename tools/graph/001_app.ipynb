{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://neptune.ai/blog/web-scraping-and-knowledge-graphs-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wikipediaapi  \n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia(name_topic, verbose=True):\n",
    "   def link_to_wikipedia(link):\n",
    "       try:\n",
    "           page = api_wikipedia.page(link)\n",
    "           if page.exists():\n",
    "               return {'page': link, 'text': page.text, 'link': page.fullurl, 'categories': list(page.categories.keys())}\n",
    "       except:\n",
    "           return None\n",
    "      \n",
    "   api_wikipedia = wikipediaapi.Wikipedia(language='en',user_agent=\"Mozilla/5.0 (X11; Linux i686; rv:109.0) Gecko/20100101 Firefox/121.0\", extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "   name_of_page = api_wikipedia.page(name_topic)\n",
    "   if not name_of_page.exists():\n",
    "       print('Page {} is not present'.format(name_of_page))\n",
    "       return\n",
    "  \n",
    "   links_to_page = list(name_of_page.links.keys())\n",
    "   procceed = tqdm(desc='Scraped links', unit='', total=len(links_to_page)) if verbose else None\n",
    "   origin = [{'page': name_topic, 'text': name_of_page.text, 'link': name_of_page.fullurl, 'categories': list(name_of_page.categories.keys())}]\n",
    "  \n",
    "   with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "       links_future = {executor.submit(link_to_wikipedia, link): link for link in links_to_page}\n",
    "       for future in concurrent.futures.as_completed(links_future):\n",
    "           info = future.result()\n",
    "           origin.append(info) if info else None\n",
    "           procceed.update(1) if verbose else None\n",
    "   procceed.close() if verbose else None\n",
    "  \n",
    "   namespaces = ('Wikipedia', 'Special', 'Talk', 'LyricWiki', 'File', 'MediaWiki',\n",
    "                 'Template', 'Help', 'User', 'Category talk', 'Portal talk')\n",
    "   origin = pd.DataFrame(origin)\n",
    "   origin = origin[(len(origin['text']) > 20)\n",
    "                     & ~(origin['page'].str.startswith(namespaces, na=True))]\n",
    "   origin['categories'] = origin.categories.apply(lambda a: [b[9:] for b in a])\n",
    "\n",
    "   origin['topic'] = name_topic\n",
    "   print('Scraped pages', len(origin))\n",
    "  \n",
    "   return origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraped links: 100%|██████████| 1593/1593 [03:11<00:00,  8.32/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped pages 1549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_wikipedia = scrape_wikipedia('Artificial_intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wikipedia.to_csv('scraped_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data stored as csv we want to create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download spacy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import requests\n",
    "from spacy import displacy\n",
    "# import en_core_web_sm\n",
    " \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    " \n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    " \n",
    "import matplotlib.pyplot as plot\n",
    "from tqdm import tqdm\n",
    "import networkx as ntx\n",
    "import neptune\n",
    " \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/7frank/scaping-knowledge-graph/e/SCAP-4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_TOKEN = os.getenv('NEPTUNE_API_TOKEN')\n",
    "\n",
    "# FIXME api token isnt laoded correctly but instead truncated\n",
    "run = neptune.init_run(\n",
    "    project=\"7frank/scaping-knowledge-graph\",\n",
    "    api_token=API_TOKEN+\"==\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to Neptune.\n",
    "run[\"data\"].upload(\"scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>link</th>\n",
       "      <th>categories</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "      <td>Artificial intelligence (AI) is the intelligen...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Artificial_intel...</td>\n",
       "      <td>['All articles needing additional references',...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023 AI Safety Summit</td>\n",
       "      <td>The AI Safety Summit was an international conf...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/AI_Safety_Summit</td>\n",
       "      <td>['2023 conferences', 'All Wikipedia articles i...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2001: A Space Odyssey (film)</td>\n",
       "      <td>2001: A Space Odyssey is a 1968 epic science f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2001:_A_Space_Od...</td>\n",
       "      <td>['1960s American films', '1960s British films'...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3D optical data storage</td>\n",
       "      <td>3D optical data storage is any form of optical...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/3D_optical_data_...</td>\n",
       "      <td>['All articles lacking in-text citations', 'Al...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A* search algorithm</td>\n",
       "      <td>A* (pronounced \"A-star\") is a graph traversal ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A*_search_algorithm</td>\n",
       "      <td>['All articles with unsourced statements', 'Ar...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>1579</td>\n",
       "      <td>Category:Wikipedia articles needing clarificat...</td>\n",
       "      <td>This category combines all Wikipedia articles ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Category:Wikiped...</td>\n",
       "      <td>['CatAutoTOC generates standard Category TOC',...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>1580</td>\n",
       "      <td>Portal:Computer programming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Portal:Computer_...</td>\n",
       "      <td>['All portals', 'All portals with triaged subp...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>1581</td>\n",
       "      <td>Portal:Philosophy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Portal:Philosophy</td>\n",
       "      <td>['All portals', 'All portals with untriaged su...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>1582</td>\n",
       "      <td>Portal:Science</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Portal:Science</td>\n",
       "      <td>['All manually maintained portal pages', 'All ...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>1583</td>\n",
       "      <td>Portal:Technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Portal:Technology</td>\n",
       "      <td>['All portals', 'All portals with triaged subp...</td>\n",
       "      <td>Artificial_intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1549 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               page  \\\n",
       "0              0                            Artificial_intelligence   \n",
       "1              1                              2023 AI Safety Summit   \n",
       "2              2                       2001: A Space Odyssey (film)   \n",
       "3              3                            3D optical data storage   \n",
       "4              4                                A* search algorithm   \n",
       "...          ...                                                ...   \n",
       "1544        1579  Category:Wikipedia articles needing clarificat...   \n",
       "1545        1580                        Portal:Computer programming   \n",
       "1546        1581                                  Portal:Philosophy   \n",
       "1547        1582                                     Portal:Science   \n",
       "1548        1583                                  Portal:Technology   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Artificial intelligence (AI) is the intelligen...   \n",
       "1     The AI Safety Summit was an international conf...   \n",
       "2     2001: A Space Odyssey is a 1968 epic science f...   \n",
       "3     3D optical data storage is any form of optical...   \n",
       "4     A* (pronounced \"A-star\") is a graph traversal ...   \n",
       "...                                                 ...   \n",
       "1544  This category combines all Wikipedia articles ...   \n",
       "1545                                                NaN   \n",
       "1546                                                NaN   \n",
       "1547                                                NaN   \n",
       "1548                                                NaN   \n",
       "\n",
       "                                                   link  \\\n",
       "0     https://en.wikipedia.org/wiki/Artificial_intel...   \n",
       "1        https://en.wikipedia.org/wiki/AI_Safety_Summit   \n",
       "2     https://en.wikipedia.org/wiki/2001:_A_Space_Od...   \n",
       "3     https://en.wikipedia.org/wiki/3D_optical_data_...   \n",
       "4     https://en.wikipedia.org/wiki/A*_search_algorithm   \n",
       "...                                                 ...   \n",
       "1544  https://en.wikipedia.org/wiki/Category:Wikiped...   \n",
       "1545  https://en.wikipedia.org/wiki/Portal:Computer_...   \n",
       "1546    https://en.wikipedia.org/wiki/Portal:Philosophy   \n",
       "1547       https://en.wikipedia.org/wiki/Portal:Science   \n",
       "1548    https://en.wikipedia.org/wiki/Portal:Technology   \n",
       "\n",
       "                                             categories  \\\n",
       "0     ['All articles needing additional references',...   \n",
       "1     ['2023 conferences', 'All Wikipedia articles i...   \n",
       "2     ['1960s American films', '1960s British films'...   \n",
       "3     ['All articles lacking in-text citations', 'Al...   \n",
       "4     ['All articles with unsourced statements', 'Ar...   \n",
       "...                                                 ...   \n",
       "1544  ['CatAutoTOC generates standard Category TOC',...   \n",
       "1545  ['All portals', 'All portals with triaged subp...   \n",
       "1546  ['All portals', 'All portals with untriaged su...   \n",
       "1547  ['All manually maintained portal pages', 'All ...   \n",
       "1548  ['All portals', 'All portals with triaged subp...   \n",
       "\n",
       "                        topic  \n",
       "0     Artificial_intelligence  \n",
       "1     Artificial_intelligence  \n",
       "2     Artificial_intelligence  \n",
       "3     Artificial_intelligence  \n",
       "4     Artificial_intelligence  \n",
       "...                       ...  \n",
       "1544  Artificial_intelligence  \n",
       "1545  Artificial_intelligence  \n",
       "1546  Artificial_intelligence  \n",
       "1547  Artificial_intelligence  \n",
       "1548  Artificial_intelligence  \n",
       "\n",
       "[1549 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv('scraped_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation\n",
    "\n",
    "The first step of building a knowledge graph is to split the text document or article into sentences. Then we limit our examples to simple sentences with one subject and one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --- det\n",
      "AbC-19 --- amod\n",
      "rapid --- amod\n",
      "antibody --- compound\n",
      "test --- nsubj\n",
      "is --- ROOT\n",
      "an --- det\n",
      "immunological --- amod\n",
      "test --- attr\n",
      "for --- prep\n",
      "COVID-19 --- compound\n",
      "exposure --- pobj\n",
      "developed --- acl\n",
      "by --- agent\n",
      "\n",
      " --- dep\n",
      "the --- det\n",
      "UK --- compound\n",
      "Rapid --- compound\n",
      "Test --- compound\n",
      "Consortium --- pobj\n",
      "and --- cc\n",
      "manufactured --- conj\n",
      "by --- agent\n",
      "Abingdon --- compound\n",
      "Health --- pobj\n",
      ". --- punct\n",
      "It --- nsubj\n",
      "uses --- ROOT\n",
      "a --- det\n",
      "lateral --- amod\n",
      "flow --- compound\n",
      "test --- dobj\n",
      "to --- aux\n",
      "determine --- xcomp\n",
      "\n",
      " --- dep\n",
      "whether --- mark\n",
      "a --- det\n",
      "person --- nsubj\n",
      "has --- ccomp\n",
      "IgG --- nsubj\n",
      "antibodies --- conj\n",
      "to --- prep\n",
      "the --- det\n",
      "SARS --- compound\n",
      "- --- punct\n",
      "CoV-2 --- compound\n",
      "virus --- pobj\n",
      "that --- nsubj\n",
      "causes --- relcl\n",
      "COVID-19 --- dobj\n",
      ". --- punct\n",
      "The --- det\n",
      "test --- nsubj\n",
      "uses --- ROOT\n",
      "a --- det\n",
      "single --- amod\n",
      "\n",
      " --- dep\n",
      "drop --- dobj\n",
      "of --- prep\n",
      "blood --- pobj\n",
      "obtained --- acl\n",
      "from --- prep\n",
      "a --- det\n",
      "finger --- compound\n",
      "prick --- pobj\n",
      "and --- cc\n",
      "yields --- conj\n",
      "results --- dobj\n",
      "in --- prep\n",
      "20 --- nummod\n",
      "minutes --- pobj\n",
      ". --- punct\n",
      "\n",
      "\n",
      " --- dep\n",
      "See --- ROOT\n",
      "also --- advmod\n",
      "\n",
      " --- dep\n",
      "COVID-19 --- nmod\n",
      "rapid --- amod\n",
      "\n",
      " --- dep\n",
      "antigen --- compound\n",
      "test --- npadvmod\n"
     ]
    }
   ],
   "source": [
    "# Lets take part of the above extracted article\n",
    "docu = nlp('''The AbC-19 rapid antibody test is an immunological test for COVID-19 exposure developed by\n",
    "the UK Rapid Test Consortium and manufactured by Abingdon Health. It uses a lateral flow test to determine\n",
    "whether a person has IgG antibodies to the SARS-CoV-2 virus that causes COVID-19. The test uses a single\n",
    "drop of blood obtained from a finger prick and yields results in 20 minutes.\\n\\nSee also\\nCOVID-19 rapid\n",
    "antigen test''')\n",
    " \n",
    "for tokn in docu:\n",
    "   print(tokn.text, \"---\", tokn.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pre-trained SpaCy model as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/freimann/miniconda3/envs/mEnv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SpaCy pipeline assigns word vectors, context-specific token vectors, part-of-speech tags, dependency parsing, and named entities. By extending SpaCy’s pipeline of annotations you can resolve coreferences (explained below).\n",
    "\n",
    "Knowledge graphs can be automatically constructed from parts-of-speech and dependency parsing. Extraction of entity pairs from grammatical patterns is fast and scalable to large amounts of text using the NLP library SpaCy.\n",
    "\n",
    "The following function defines entity pairs as entities/noun chunks with subject-object dependencies connected by a root verb. Other approximations can be used to produce different types of connections. This kind of connection can be referred to as subject-predicate-object triple.\n",
    "\n",
    "The main idea is to go through a sentence and extract the subject and object, and when they’re encountered. The below function has some of the steps mentioned.\n",
    "Entity extraction\n",
    "\n",
    "You can extract a single-word entity from a sentence with the help of parts-of-speech (POS) tags. The nouns and proper nouns will be the entities. \n",
    "\n",
    "However, when an entity spans multiple words, POS tags alone aren’t sufficient. We need to parse the dependency tree of the sentence. To build a knowledge graph, the most important things are the nodes and edges between them. \n",
    "\n",
    "These nodes are going to be entities that are present in the Wikipedia sentences. Edges are the relationships connecting these entities. We will extract these elements in an unsupervised manner, i.e. we’ll use the grammar of the sentences.\n",
    "\n",
    "The idea is to go through a sentence and extract the subject and the object as and when they are reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(sents):\n",
    "   # chunk one\n",
    "   enti_one = \"\"\n",
    "   enti_two = \"\"\n",
    "  \n",
    "   dep_prev_token = \"\" # dependency tag of previous token in sentence\n",
    "  \n",
    "   txt_prev_token = \"\" # previous token in sentence\n",
    "  \n",
    "   prefix = \"\"\n",
    "   modifier = \"\"\n",
    "  \n",
    "  \n",
    "  \n",
    "   for tokn in nlp(sents):\n",
    "       # chunk two\n",
    "       ## move to next token if token is punctuation\n",
    "      \n",
    "       if tokn.dep_ != \"punct\":\n",
    "           #  check if token is compound word or not\n",
    "           if tokn.dep_ == \"compound\":\n",
    "               prefix = tokn.text\n",
    "               # add the current word to it if the previous word is 'compound’\n",
    "               if dep_prev_token == \"compound\":\n",
    "                   prefix = txt_prev_token + \" \"+ tokn.text\n",
    "                  \n",
    "           # verify if token is modifier or not\n",
    "           if tokn.dep_.endswith(\"mod\") == True:\n",
    "               modifier = tokn.text\n",
    "               # add it to the current word if the previous word is 'compound'\n",
    "               if dep_prev_token == \"compound\":\n",
    "                   modifier = txt_prev_token + \" \"+ tokn.text\n",
    "                  \n",
    "           # chunk3\n",
    "           if tokn.dep_.find(\"subj\") == True:\n",
    "               enti_one = modifier +\" \"+ prefix + \" \"+ tokn.text\n",
    "               prefix = \"\"\n",
    "               modifier = \"\"\n",
    "               dep_prev_token = \"\"\n",
    "               txt_prev_token = \"\"\n",
    "              \n",
    "           # chunk4\n",
    "           if tokn.dep_.find(\"obj\") == True:\n",
    "               enti_two = modifier +\" \"+ prefix +\" \"+ tokn.text\n",
    "              \n",
    "           # chunk 5\n",
    "           # update variable\n",
    "           dep_prev_token = tokn.dep_\n",
    "           txt_prev_token = tokn.text\n",
    "          \n",
    "   return [enti_one.strip(), enti_two.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rapid antibody test', 'immunological UK Rapid Test']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entities(\"The AbC-19 rapid antibody test is an immunological test for COVID-19 exposure developed by the UK Rapid Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let’s use the function to extract entity pairs for 800 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [14:42<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "pairs_of_entities = []\n",
    "for i in tqdm(data['text'][:800]):\n",
    "   pairs_of_entities.append(extract_entities(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Such DCAS regulation', 'External Highway Safety'],\n",
       " ['External Faith links', 'Computer Faith Sciences'],\n",
       " ['Taxonomy Machine Learning Workshop', '12567278 Artificial Intelligence'],\n",
       " ['typically Chemical rockets', 'Hierarchical Aeronautics Europe'],\n",
       " ['which', 'Power Planetary Aerobots Systems'],\n",
       " ['R. B. R. Preferences', '2008 Affect Affect'],\n",
       " ['social  that', 'sensitive Internet PhilPapers'],\n",
       " ['also Citations Works', 's1071 Affective Computing'],\n",
       " ['extreme CNC Farming VAE', 'Agricultural Wikimedia Commons'],\n",
       " ['ITAC Canadian AI He', 'Royal Canada'],\n",
       " ['Notable engineering people', 'Official Alan Turing Institute'],\n",
       " ['early AlexNet paper', '120,000 Google Scholar'],\n",
       " ['neural Turing he', 'paper']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subject object pairs from sentences\n",
    "pairs_of_entities[36:42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relations extraction\n",
    "\n",
    "With entity extraction, half the job is done. To build a knowledge graph, we need to connect the nodes (entities). These edges are relations between pairs of nodes. The function below is capable of capturing such predicates from these sentences. I used spaCy’s rule-based matching. The pattern defined in the function tries to find the ROOT word or the main verb in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_relation(sent):\n",
    "  \n",
    "   doc = nlp(sent)\n",
    "  \n",
    "   matcher = Matcher(nlp.vocab)\n",
    "  \n",
    "   pattern = [{'DEP':'ROOT'},\n",
    "           {'DEP':'prep','OP':\"?\"},\n",
    "           {'DEP':'agent','OP':\"?\"}, \n",
    "           {'POS':'ADJ','OP':\"?\"}]\n",
    "  \n",
    "   matcher.add(\"matching_1\", [pattern])\n",
    "  \n",
    "   matcher = matcher(doc)\n",
    "   h = len(matcher) - 1\n",
    "  \n",
    "   span = doc[matcher[h][1]:matcher[h][2]]\n",
    "  \n",
    "   return span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern which is written above tries to find the root word in sentences. Once it is recognized then it checks if a preposition or an agent word follows it. If it’s a yes then it’s added to the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 276/800 [02:50<08:51,  1.02s/it]"
     ]
    }
   ],
   "source": [
    "relations = [obtain_relation(j) for j in tqdm(data['text'][:800])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
