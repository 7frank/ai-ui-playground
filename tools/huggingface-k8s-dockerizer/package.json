{
  "name": "huggingface-k8s-dockerizer",
  "module": "index.ts",
  "type": "module",
  "devDependencies": {
    "@types/bun": "latest"
  },
  "peerDependencies": {
    "typescript": "^5.0.0"
  },
  "dependencies": {
    "@pulumi/kubernetes": "^4.9.0",
    "chalk": "^5.3.0",
    "cmd-ts": "^0.13.0",
    "enquirer": "^2.4.1",
    "fast-glob": "^3.3.2",
    "js-yaml": "^4.1.0"
  },
  "scripts": {
    "d-build-base": "docker build -t frank1147/huggingface-gradio-base -f base/Dockerfile.huggingface-gradio-base .",
    "d-push-base": "docker push frank1147/huggingface-gradio-base:latest",
    "d-bash-base": "docker run --entrypoint /bin/bash -it frank1147/huggingface-gradio-base",
    "build-lf": "docker build -t frank1147/langflow-custom-components -f base/Dockerfile.langflow .",
    "start-lf": "docker run --env-file .env -v $(pwd)/langflow/db:/app/db -d --memory 500m -p 8080:7860 --name langflow-custom-components frank1147/langflow-custom-components",
    "logs-lf": "docker ps -a --filter 'ancestor=frank1147/langflow-custom-components' --format  '{{.ID}}'  | xargs -I {} docker logs --follow  {}",
    "stop-lf": "docker stop langflow-custom-components && docker rm langflow-custom-components",
    "push-lf": "docker push frank1147/langflow-custom-components:latest",
    "attach-lf": "docker exec -it langflow-custom-components bash",
    "bash-lf": "docker run -v $(pwd)/langflow/db:/app/db -p 8080:7860 --entrypoint /bin/bash -it frank1147/langflow-custom-components",
    "build-vllm": "docker build -t frank1147/vllm -f base/Dockerfile.vllm .",
    "start-vllm": "docker run --env-file .env -v $(pwd)/langflow/db:/app/db -d --memory 500m -p 8080:7860 --name vllm frank1147/vllm",
    "logs-vllm": "docker ps -a --filter 'ancestor=frank1147/vllm' --format  '{{.ID}}'  | xargs -I {} docker logs --follow  {}",
    "stop-vllm": "docker stop vllm && docker rm vllm",
    "push-vllm": "docker push frank1147/vllm:latest",
    "attach-vllm": "docker exec -it vllm bash",
    "bash-vllm": "docker run -v $(pwd)/langflow/db:/app/db -p 8080:7860 --entrypoint /bin/bash -it frank1147/vllm",
    "build-ollama": "docker build -t frank1147/ollama-gpu -f base/Dockerfile.ollama-gpu .",
    "start-ollama": "docker run -v /usr/share/ollama/.ollama/models:/root/.ollama/models -d --memory 6g -p 8080:11434 --name ollama frank1147/ollama-gpu",
    "logs-ollama": "docker ps -a --filter 'ancestor=frank1147/ollama-gpu' --format '{{.ID}}' | xargs -I {} docker logs --follow {}",
    "stop-ollama": "docker stop ollama && docker rm ollama",
    "push-ollama": "docker push frank1147/ollama-gpu:latest",
    "attach-ollama": "docker exec -it ollama bash",
    "bash-ollama": "docker run -v /usr/share/ollama/.ollama/models:/root/.ollama/models -p 8080:11434 --entrypoint /bin/bash -it frank1147/ollama-gpu",
    "list-services": "kubectl get ingress -o=jsonpath='{range .items[*]}{.spec.rules[*].host}{\"\n\"}{end}'"
  }
}
