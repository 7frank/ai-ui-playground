---
title: Artificial neural networks (ANN)
description: Overview of different neuronal networks
---

## Convolutional Neural Networks (CNNs):

[CNN Overview](/ai-ui-playground/ai/convolutional-neural-networks-cnn/overview/)

- embedding
  - turn a "something" into a number that is associated with a list of numbers that represent its properties (automatically trained in our case)
  - initially random
  - change through training

## Large Language Models (LLMs) like GPT-3, GPT-4:

[LLM Overview](/ai-ui-playground/ai/llm/overview/)

### transformers

[LLM Overview](/ai-ui-playground/ai/llm/transformers/)

## Recurrent Neural Networks (RNNs):

    Use Case: Suited for sequential data such as time series, speech, and text. They are used in tasks like speech recognition, language modeling, and machine translation.
    Key Features: Have loops in their architecture to maintain information in 'memory' over time, allowing them to process sequences of data.

## Long Short-Term Memory Networks (LSTMs):

    Use Case: A more advanced form of RNNs, effective at learning long-range dependencies in sequential data. Commonly used in complex sequence modeling tasks like language translation and speech synthesis.
    Key Features: Include special units called memory cells that can maintain information in memory for long periods, addressing the vanishing gradient problem common in standard RNNs.

## Autoencoders:

    Use Case: Used for unsupervised learning tasks, such as dimensionality reduction, feature learning, and generative models. They are particularly useful in image denoising, anomaly detection, and data compression.
    Key Features: Consist of an encoder that compresses the input and a decoder that reconstructs the input from the compressed representation.

## Generative Adversarial Networks (GANs):

    Use Case: Widely used in generative tasks like image generation, image super-resolution, and style transfer. They have also found applications in data augmentation and synthetic data generation.
    Key Features: Comprise two parts â€“ a generator that creates samples and a discriminator that distinguishes between generated and real samples, trained simultaneously in a competitive manner.

## Graph Neural Networks (GNNs):

    Use Case: Designed for data that is represented as graphs, such as social networks, molecule structures, and recommendation systems.
    Key Features: Capable of capturing the dependencies in graph structures, they update the representation of a node based on its neighbors.

## Transformer Networks:

    Use Case: Beyond their use in language models, transformers are also adapted for various tasks like image recognition (Vision Transformers, ViT) and multimodal tasks (combining text and images).
    Key Features: Relies on self-attention mechanisms to process sequences of data, whether it's text, image patches, or other forms of structured data.

## other

- Group method of data handling (GMDH)
- Probabilistic(PNN)
- time delay neural network (TDNN)
- deep stacking network (DSN)

and may more - [Types of artificial neural networks](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks)

## thesaurus

- parameters
- epoch
  - backpropagation & updating embeddings
- backpropagation
- feed forward
- bias term
