---
title: OtherMethods
---

## Ensemble learning

Ensemble learning is a machine learning paradigm where multiple models (often called "weak learners") are trained to solve the same problem and combined to get better results. The main premise is that by combining several models, the ensemble can achieve higher accuracy and overcome the limitations of individual models. This approach is particularly effective in reducing overfitting, enhancing predictive performance, and dealing with complex data structures. For more details, you can visit the Wikipedia page on [Ensemble Learning](https://de.wikipedia.org/wiki/Ensemble_learning).

- **Random Forest**: A machine learning algorithm used for classification and regression that operates by constructing multiple decision trees during the training phase.

  - https://de.wikipedia.org/wiki/Random_Forest

- **AdaBoost**: A machine learning algorithm that combines multiple weak learners to create a strong classifier. It's commonly used to boost the performance of decision trees.
  - https://de.mathworks.com/discovery/adaboost.html

## Gradient boosting

Gradient boosting frameworks are advanced machine learning techniques used to predict outcomes by combining multiple weak prediction models, typically decision trees. They iteratively add models to the ensemble, focusing on correcting errors made by previous models. These frameworks are highly efficient, scalable, and can handle various types of data, making them suitable for a wide range of applications, including classification, regression, and ranking tasks. They are known for their ability to improve predictive accuracy and control over-fitting through parameters tuning.

- **XGBoost**: An efficient and scalable implementation of gradient boosting framework that's used for supervised learning problems.
  - https://xgboost.readthedocs.io/en/stable/
- **LightGBM**: A gradient boosting framework that uses tree-based learning algorithms, designed for **efficiency and speed**.
  - https://lightgbm.readthedocs.io/en/stable/
- **CatBoost**: An open-source machine learning algorithm that uses gradient boosting on decision trees, designed to handle categorical data automatically and reduce overfitting.
  - https://catboost.ai/

## MLP 

- Multi-Layer Perceptron (MLP): A class of feedforward artificial neural network (ANN) that consists of at least three layers of nodes and is used for classification and regression tasks.
  - https://de.wikipedia.org/wiki/Multi-Layer-Perzeptron
