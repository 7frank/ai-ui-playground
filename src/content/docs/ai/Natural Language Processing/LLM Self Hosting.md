---
title: LLM - Self Hosting
---


## Self Hosting

### Why
- customers and developers sometimes can't share data with openai and other services
- see [CodeJeeves](../Potential%20Use%20Cases/CodeJeeves.mdx) ideas section for more
### Use cases


- Self Hosted LLM as a Service
- "White Label"
- internal API that serves as a project kickstarter by making retrieving access to LLMs as easy as possbile
  - no data exposed
  - no setup of LLM per developer

### building blocks
- [openai api quota proxy](https://github.com/AI-Northstar-Tech/openai-proxy
- [Local.AI](https://localai.io/)
- Docker
- onPrem or Cloud GPU
- (K8s for scaling and distributing GPU)



